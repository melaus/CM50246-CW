\documentclass[11pt,openright,a4paper]{article}
\usepackage{fancyhdr}
\usepackage{datetime}
\usepackage[left=25mm,right=25mm,top=35mm,bottom=35mm,footskip=0.5in]{geometry}
% \include{defs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amsbsy}
\usepackage{graphicx}
\usepackage{float}

\usepackage[english]{babel}
\usepackage[backend=bibtex,sorting=none,style=ieee]{biblatex}
\bibliography{task1bib}

\graphicspath{ {images/} }
\usepackage{graphicx}

% header
\pagestyle{fancy}
\fancyhf{}
\lhead{CM50426 Machine Learning \& AI \\ Alan Lau}
\rhead{Task 1 \\ Semester 1, 2016-17}
\cfoot{\thepage}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\numberwithin{equation}{section}

\abovedisplayskip=12pt plus 3pt minus 9pt
\abovedisplayshortskip=0pt plus 3pt
\belowdisplayskip=12pt plus 3pt minus 9pt
\belowdisplayshortskip=7pt plus 3pt minus 4pt

\usepackage[font=small,labelfont=bf]{caption}

\begin{document}

%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%
Unsupervised learning algorithms attempt to learn the hidden structure of a given dataset without any labels or knowledge of the distributions of the data. A prominent usage of such algorithms is for clustering. A given set of datapoints is split into groups based on their underlying patterns. By learning the required parameters, points can be assigned to clusters and distributions can be observed.

Broadly speaking, there are three types of clustering algorithms, namely, centroid-based, distribution-based and density-based. A centroid-based algorithm, such as \textit{K}-means, uses a single mean vector to represent each cluster, and assign points to clusters in a binary fashion. On the other hand, Gaussian Mixture Model (GMM) is a prime example of a distribution-based clustering algorithm. It mixes together multivariate Gaussian distributions and uses probability theories to derive soft assignments to the corresponding distributions. 

We shall further discuss \textit{K}-Means and GMM, and a generic method called Expectation-Maximisation that facilitates these two algorithms.


%%%%%%%%%%%%
\section{Expectation-Maximisation (EM)} \label{sec:em}
%%%%%%%%%%%%

\begin{equation} \label{eq:em-prob}
    Pr\left( \boldsymbol{x} | \boldsymbol{\theta} \right) = 
    \int Pr\left( \boldsymbol{x},\boldsymbol{h} | \boldsymbol{\theta} \right) \; d\boldsymbol{h}
\end{equation}

\begin{equation} \label{eq:em-max}
    \hat{\boldsymbol{\theta}} = 
        \underset{\boldsymbol{\theta}}{\mathrm{argmax}}
        \left[ \sum_{i=1}^{I} \log \left[ 
            \int Pr\left( \boldsymbol{x}_{i},\boldsymbol{h}_{i} | \boldsymbol{\theta} \right) \; d\boldsymbol{h}_{i} 
            \right] 
        \right]
\end{equation}

EM is a generic algorithm that is used to fit parameters, $\boldsymbol{\theta}$, of models in the form of \autoref{eq:em-prob}. The algorithm aims to find the arguments that maximises the marginalised joint probability facilitated by the hidden variable, $\boldsymbol{h}$, as shown in \autoref{eq:em-max}. One could directly apply maximum likelihood (ML) and find $\boldsymbol{\theta}$ by maximising the log likelihood of $Pr\left( \boldsymbol{x}| \boldsymbol{\theta} \right)$. This eliminates the need of $\boldsymbol{h}$ which makes the calculation more straight-forward, but no closed form solution will be found for the models we are concerned of.

It is a two-step process - \textit{E-step} and \textit{M-step}:
\begin{itemize}
    \item \textit{E-step} (expectation step) updates the probabiliy distributions, $q_{i}\left(\boldsymbol{h}_{i} \right)$, while fixing $\boldsymbol{\theta}$ to maximise the lower bound; and,
    \item \textit{M-step} (maximisation step) updates the parameters, $\boldsymbol{\theta}$, while fixing $q_{i}\left(\boldsymbol{h}_{i} \right)$ to maximise the lower bound. 
\end{itemize}

The E-step and M-step runs in this order for once each in every iteration. The algorithm will iterate until it converges , which is guaranteed by the bounds of the algorithm. In other words, the algorithm will iterate until a local maximum in the likelihood function is reached. The goal for both steps is to increase the lower bound $\mathcal{B}\left[q_{i}\left(\boldsymbol{h}_i\right),\boldsymbol{\theta} \right]$ while not exceeding the upper bound, which is the sum of the log joint likelihoods of a point and its corresponding $\boldsymbol{h}$ value. 


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{em-overview}
    \caption{(left) The graph shows an E-step, where manipulating the probability distributions changes all values of the lower bound for every $\boldsymbol{\theta}$, hence the whole curve moves towards the red curve, which is the actual log likelihood curve. (right) The graph shows an M-step, where the $\boldsymbol{\theta}$ is changed while fixing $q_{i}(\boldsymbol{h}_{i}$ to improve the estimate from the blue dot to the purple dot. (Figure taken from \cite{prince}.)}
  \label{fig:em-overview}
\end{figure} 


%%%%%%%%%%%%
\section{\textit{K}-means} \label{sec:km}
%%%%%%%%%%%%
The goal of \textit{K}-means is that given $K$ number of clusters, assign each point to a cluster centre at this iteration (known as a prototype) , $\boldsymbol{\mu}_k$, where the distance between the point and the this centre is the smallest compared to the others. The means of the points assigned to each cluster form the new prototypes, and the process continues until it converges. This resembles the EM structure where we first fix the prototypes to update responsibilities, then the other way round.

\begin{equation} \label{eq:km-obj-func}
    J = \sum_{i=1}^{I} \sum_{k=1}^{K} r_{ik} \left|\left| \boldsymbol{x}_{i} - \boldsymbol{\mu}_{k} \right|\right|^2
\end{equation}

More formally, we define the objective function of the model as \autoref{eq:km-obj-func}, with $r_{ik}$ being the responsibility and the latter part the Euclidean distance between the datapoint and prototype concerned. We aim to minimise this function through the EM procedures.

\begin{equation} \label{eq:km-e-step}
    r_{ik} = 
    \begin{cases}
        1 \quad \mathrm{if} \, k = \mathrm{argmin}_j
                            \left|\left|\boldsymbol{x}_{i} - \boldsymbol{\mu}_{k} \right|\right| \\
        0 \quad \mathrm{otherwise}
    \end{cases}
\end{equation}

In the E-step, we update the responsibilities while fixing the prototypes. Here, the responsibilities act as `weights', telling how likely a datapoint belongs to each of the clusters. The sum of all responsibilities for each datapoint across all clusters should always equal to 1. In fact, for \textit{K}-means, this is simply a binary assignment, as datapoints are hard-assigned to one cluster only, as illustrated in \autoref{eq:km-e-step}.

\begin{equation} \label{eq:km-m-step}
    \boldsymbol\mu_{k} = \frac{\sum_i r_{ik}\boldsymbol{x}_i}{\sum_i r_{ik}}
\end{equation}

In the M-step, using the new responsibility assignments from the E-step, we calculate the prototypes, as shown in \autoref{eq:km-m-step}. Essentially, it is simply the mean of all the datapoints of a cluster.

The EM steps are repeated until the prototypes remains unchanged. Then, the final centroids are found. The dataset is grouped together by them, for which they act as the means of the $K$ different clusters.

%TODO: How to initialise 


%%%%%%%%%%%%
\section{Guassion Mixture Model (GMM)} \label{sec:gmm}
%%%%%%%%%%%%

% TODO
% responsibilities
% How K-means is a specific case of GMM
% why EM but not direct ML
% what is it good for 

\begin{equation}
    \label{eq:norm-dist}
    \mathrm{Norm}_{x}\left[\boldsymbol{\mu},\boldsymbol{\Sigma}\right] = 
        \frac{1}{\left( 2 \pi \right)^\frac{d}{2}}
        |\boldsymbol\Sigma|^{-\frac{1}{2}} 
        \,\mathrm{exp}\, 
        \left\{-
            \frac{1}{2}
            \left(\boldsymbol{x}-\boldsymbol{\mu}\right)^\mathrm{T} 
            \boldsymbol{\Sigma}^{-1}
            \left(\boldsymbol{x}-\boldsymbol{\mu}\right)
        \right\}
\end{equation}

\begin{equation} \label{eq:gmm}
    Pr\left(\boldsymbol{x} | \boldsymbol{\theta}\right) =
    \sum_{k=1}^K \lambda_k \mathrm{Norm}_{x}\left[\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right]
\end{equation}

We define the multivariate normal distribution in \autoref{eq:norm-dist}, with means, $\boldsymbol\mu$, and covariance matrices, $\boldsymbol\Sigma$, as parameters. A mixture of Gaussian is the weighted sum of all $K$ normal distributions, also known as linear super-position of Gaussians, as illustrated by \autoref{eq:gmm}. Here, $K$ represents the number of components given to fit the the model to the data. The weights, $\lambda$, can also be viewed as the priors to the probability.

% TODO: add Cat[lambda] stuff

A mixture of Gaussian poses some constraints. Like $K$-means, the weights, $\lambda$, for each datapoint across all components should sum to 1, which $Pr\left(\boldsymbol{x}\right)$. Also, $\boldsymbol{\Sigma}$ needs to be positive definite matrices for $\mathrm{Norm}_x$ to work.

As mentioned in \autoref{sec:em}, it is not possible to obtain a closed form solution when differentiate with respect to $\boldsymbol{\theta}$ and equating that to zero by directly applying maximum likelihood. A non-linear optimisation approach could be attempted, but it would be rather difficult to maintain the aforementioned parametric contraints. The EM approach provides a simple yet powerful method to fit against a multi-dimensional dataset. 

\begin{equation} \label{eq:gmm-back}
    \begin{aligned}
        Pr\left(\boldsymbol{x}|\boldsymbol{h},\boldsymbol{\theta}\right) 
            &= \mathrm{Norm}_x\left[\boldsymbol{\mu}_h,\boldsymbol{\Sigma}_h\right] \\ 
        Pr(\boldsymbol{h}|\boldsymbol{\theta}) &= \mathrm{Cat}_h\left[ \boldsymbol{\lambda}\right]
    \end{aligned}
\end{equation}

Going back to \autoref{eq:gmm} - the probability distribution is derived by considering the hidden variable, $\boldsymbol{h}$. By defining \autoref{eq:gmm-back}, we can define $Pr(\boldsymbol{x} | \boldsymbol{\theta})$ as the marginalisation between $\boldsymbol{x}$ and $\boldsymbol{\lambda}$.

In this way, we can draw samples using $Pr(\boldsymbol{x},h)$. Discarding $h$ leaves the samples. We also observe that the hidden variable here is the weights for the model, which describes the responsibilities to each normal distribution for each datapoint. 

\subsection{Applying EM to GMM} \label{ssec:gmm-em}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{gmm-e-step}
    \caption{By calculating the posterior distribution, $Pr(h_i|\boldsymbol{x}_i$, we obtain a set of responsibilities. In this example, we can see that component 1 has a much higher responsibility for the datapoint $\boldsymbol{x}_i$ than component 2. (Taken from \cite{prince})}
  \label{fig:gmm-e-step}
\end{figure} 


\begin{equation}
    \label{eq:gmm-e-step}
    q_i\left(h_i\right) = Pr\left(h_i=k|\boldsymbol{x}_i, \boldsymbol{\theta}^{\left[t\right]}\right) = 
    r_{ik} = 
    \frac{\lambda_{k}\,\mathrm{Norm}_{x_i}\left[\boldsymbol{\mu_k},\boldsymbol{\Sigma_k} \right]}
        {\sum_{k'} \lambda_{k'}\,\mathrm{Norm}_{x_i}\left[\boldsymbol{\mu_{k'}},\boldsymbol{\Sigma_{k'}} \right]}
\end{equation}

There is now sufficient information for us to consider the EM approach to learn these parameters, namely, responsibilities, means and covariance matrices under the GMM. In the E-step, we are to calculate the responsibilities. Recall that in this step, we are to evalutate the probability distributions while fixing the other parameters and the datapoint $\boldsymbol{x}_i$. 

As shown in \autoref{eq:gmm-e-step}, it is in fact the posterior probability distribution $Pr(h_i|\boldsymbol{x}_i)$ that is being calculated. Unlike $K$-Means, these are not binary (i.e. not hard) assignments. The points are softly assigned to coponents, in that the responsibility from each component is taken into account. These probabilities will help decide which component $k$ is most likely to be responsible for the datapoint $\boldsymbol{x}_i$.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{gmm-m-step}
    \caption{Observe that as the parameters are updated in the M-step, the shape distributions changes, and changing the final mixture model. (Taken from \cite{prince})}
  \label{fig:gmm-m-step}
\end{figure} 

\begin{equation}
    \label{eq:gmm-m-step}
    \begin{aligned}
        m_{k} &= \sum_{i=1}^{I} r_{ik} \\ 
        \lambda_{k}^{\left[t+1\right]} &= \frac{m_{k}}{\sum_{j=1}^{K} m_{j}} \\
        \boldsymbol\mu_{k}^{\left[t+1\right]} &= \frac{\sum_{i=1}^{I} r_{ik} \boldsymbol{x}_{i}}{m_{k}} \\
        \boldsymbol\Sigma_{k}^{\left[t+1\right]} &= 
            \frac{\sum_{i=1}^{I} r_{ik} 
                    \left(\boldsymbol{x}_{i} - \boldsymbol{\mu}_{k}^{\left[t+1\right]}\right)
                    \left(\boldsymbol{x}_{i} - \boldsymbol{\mu}_{k}^{\left[t+1\right] }\right)^\mathrm{T}}
                {m_{k}}
    \end{aligned}
\end{equation}

In the M-step, we are to update all the parameters based on the newly calculated responsibilies in the E-step, as illustrated in \autoref{eq:gmm-m-step}, which can be interpreted easily. By tweaking the parameters, shown in \autoref{fig:gmm-m-step}, the shape of the individual normal distributions change, which in turns changes the overall mixture. 

The responsibilies is crucial in updating the parameters. The influence of a datapoint on each component depends on them. For instance, the cluster means, $\boldsymbol\mu_{1...k}$, the weighted mean over the datapoints are computed, where these weights are the corresponding component responsibilities. Similarly, this is true for the covariance matrices.

The closed form solutions, the guarantee observations of the constraints on parameters and the ability to workaround missing data makes EM a great method to be used by GMM. As mentioned before, simply by applying maximum likelihood would not create closed form solutions, which makes it very difficult to observe the contraints while trying to achieve the maximum likelihood.

\newpage
%%%%%%%%%%%%%
\section{Discussions} \label{sec:dis}
%%%%%%%%%%%%%
\subsection{Linking $K$-means and GMM} \label{ssec:dis-link}
The EM framework works very well under $K$-means and GMM. It provides closed form solutions and guarantees convergence for both algorithms. EM aside, one can see that there are striking similarities between the two. The goal for both is to create clusters based on the datapoints, using a very similar set of parameters.

One could suggest that $K$-means is a specific case of GMM, in that the hard assignment and the lack of considerations of covariance makes it bias towards far apart, spherical-based datasets. On the other hand, GMM provides a more flexible solution where each cluster takes into account all datapoints. The bias is $K$-means is solved by the addition of covariance matrices as a parameter in GMM. This enables a tilted, eclipse shape for each cluster.

However, GMM suffers from dimensionality due to the use of covariance matrices. Given $n$-dimensional datapoints, the covariance matrices becomes $n*n$. For example, a $60*60$ RGB image has a dimensional vector of 10800. This means the covariance becomes $10800 * 10800$, which is rather large and computationally quite heavy. Note that such an image is relatively small in modern standard, so we can see that GMM becomes very complex very quickly as the dimensions of data increases.

$K$-Means suffer a similar problem. As the dimensions increase, the Euclidean distances is inflated as well. We only know that we prefer distances that are smallest and closest to 0, and assign points based on this distance to clusters.

Picking the appropriate value for the number of clusters ($k$) is also an issue. It is a ambiguous problem where there is no principled statistcal method available to decide for an optimal $k$. However, the elbow method, average silhouette method and gap statistics are a few strategies to help find a better informed approximate. The general idea is to run $K$-means with different number of centroids, and then evalute outcome (the within-cluster sum of sqaure, for example) to find a close approximate. The alternative is to use other clustering alogirhtms such as DBSCAN that do not require a predefined number-of-cluster value.

% TODO: if there's time
% In terms of clustering together non-flat geometry and uneven cluster sizes, DBSCAN offers a great algorithm to locate clusters through analysing the underlying density of data. It 

In \autoref{sec:results}, we will look at the impact of an increased number clusters and dimensions of the datapoints in terms of number of iterations required to reach a convergence.

\subsection{Initialisation} \label{ssec:dis-init}
One important problem to consider is how to initialise either of the algorithms. My initialisation for $K$-means is simply a uniformly random select of the given number of required clusters, $c_{1...k}$.  However, $K$-means++ \cite{kmeans-pp} introduces a better way to initialise the process, which should be pursued in the future. 
My random initialisation for $K$-means is simply a uniformly random select of the given number of required clusters, $c_{1...k}$.

Initially, point $c_1$ is picked uniformly at random from the data as the first centroid. Then, iteratively, pick the next centroid, $c_k$, from the data with probability $\frac{D(x')^2}{\sum_{i} D(x_i)^2}$ with $D$ being the D-weighting function. The initialisation method aims to find starting points from different clusters from the start, so fewer iterations are required to find the solution.

For GMM, there are two common initialisation strategies - uniformly at random and using $K$-means as a staring point. This implementation contains both of these initialisation methods. Using the $K$-means results as the starting means for GMM enable have a rough idea where these clusters may actually be. On the other hand, a random initiation, suffers the same issue as $K$-means has faced, in that these starting points may be very far out to where exactly these centres should be, which entails extra time or very unreasonable results. 

My implementation of a random initialisation works as follow: a random sample of $\frac{\text{total number of points}}{\text{number of clusters}}$ is picked for each supposed cluster. 

% TODO: how to pick k
% There is also the problem of choosing an appropriate number of clusters, $k$. In fact, it is an ambiguous problem

We shall see how these strategies compare with each other in \autoref{sec:results}.


%%%%%%%%%%%%%
\section{Results} \label{sec:results}
%%%%%%%%%%%%%
% scalability
% gmm - init random VS init with K-means

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{screen-ori-data}
    \caption{The diagram shows the three datasets being used to test the algorithms.}
  \label{fig:screen-ori-data}
\end{figure} 

Three sets of data of varying number of clusters and datapoints are used to look at the strength and weaknesses of $K$-Means and GMM. The following shows the some numeric details about these datasets:
\begin{itemize}
    \item \textbf{pts\_0}: Dimension of datapoints: 2, number of clusters: 2, number of datapoints: 500
    \item \textbf{pts\_1}: Dimension of datapoints: 2, number of clusters: 5, number of datapoints: 500
    \item \textbf{pts\_2}: Dimension of datapoints: 2, number of clusters: 10, number of datapoints: 1000
\end{itemize}

\subsection{$K$-Means} \label{sec:results-km}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{screen-km-2-2-500}
    \caption{The diagram shows K-means running on pts\_0, with the original data on the left. Terminated in 3 iterations.}
  \label{fig:screen-ori-data}
\end{figure} 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{screen-km-2-5-500}
    \caption{The diagram shows K-means running on pts\_1, with the original data on the left. Terminated in 8 iterations.}
  \label{fig:screen-screen-km-2-5-500}
\end{figure} 

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{screen-km-2-10-5000}
    \caption{The diagram shows K-means running on pts\_2, with the original data on the left. Terminated in 34 iterations.}
  \label{fig:screen-km-2-10-5000}
\end{figure} 

In the diagrams above, the limitation of K-means can be seen as the dataset becomes more complex and closer packed together. $K$-means got confused especially when the clusters are very closed together, as seen in \autoref{fig:screen-km-2-10-5000}. 

Here, it also illustrates the importance of initialisation. Our random initialisation caused some unexpected clustering to happen, forming multiple groups at where we would expect to be one big cluster. The results of $K$-means varies vastly especially when using a random initialisation. Implementing a strategy such as $K$-Means++ should provide more consistent results with better clustering.

On the other hand, given our datasets, K-means seems to be rather scalable, as all three tests ran in an expected increase in the number of iterations, and computed within a reasonable time.

\subsection{GMM} \label{ssec:results-GMM}
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{screen-gmm-2-2-500}
    \caption{This diagram shows a noticable lower number iterations required to perfrom GMM fitting with a K-means initialisation, as opposed to a random initialisation. (left) Terminated in 26 GMM iterations. (right) Terminated in 4 K-Means interations and 21 GMM iterations.}
  \label{fig:screen-gmm-2-2-500}
\end{figure} 

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{screen-gmm-2-5-500}
    \caption{This diagram shows a more complex GMM fitting. (left) 267 GMM iterations, (right) 7 K-means and 114 GMM iterations}
  \label{fig:screen-gmm-2-5-500}
\end{figure} 

The gain of using covariance as a parameter allows more flexible eclipse shapes in the datasets, but also impacted in the time and computation required. GMM took considerably more iterations and time to converge than K-Means. Also, the computational power required increased much more rapidly as the dataset gets bigger and more complex.

On the other hand, K-means initialisation vastly improves the GMM algorithm. As seen in \autoref{fig:screen-gmm-2-5-500}, many fewer expensive GMM iterations are required, and more closely resembles the original data.

\textit{See the jupyter notebook file (plots.ipynbk) for the original plots. The scripts, kmeans.py and gmm.py can both be run on their own with command line arguments. See the 'argparse' section in the code to see the available parameters.}


\printbibliography

\end{document}
